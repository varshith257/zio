"use strict";(self.webpackChunkzio_site=self.webpackChunkzio_site||[]).push([[64077],{15680:(e,n,t)=>{t.d(n,{xA:()=>m,yg:()=>f});var r=t(96540);function a(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function o(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);n&&(r=r.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,r)}return t}function i(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?o(Object(t),!0).forEach((function(n){a(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function s(e,n){if(null==e)return{};var t,r,a=function(e,n){if(null==e)return{};var t,r,a={},o=Object.keys(e);for(r=0;r<o.length;r++)t=o[r],n.indexOf(t)>=0||(a[t]=e[t]);return a}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)t=o[r],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(a[t]=e[t])}return a}var c=r.createContext({}),p=function(e){var n=r.useContext(c),t=n;return e&&(t="function"==typeof e?e(n):i(i({},n),e)),t},m=function(e){var n=p(e.components);return r.createElement(c.Provider,{value:n},e.children)},u="mdxType",l={inlineCode:"code",wrapper:function(e){var n=e.children;return r.createElement(r.Fragment,{},n)}},g=r.forwardRef((function(e,n){var t=e.components,a=e.mdxType,o=e.originalType,c=e.parentName,m=s(e,["components","mdxType","originalType","parentName"]),u=p(t),g=a,f=u["".concat(c,".").concat(g)]||u[g]||l[g]||o;return t?r.createElement(f,i(i({ref:n},m),{},{components:t})):r.createElement(f,i({ref:n},m))}));function f(e,n){var t=arguments,a=n&&n.mdxType;if("string"==typeof e||a){var o=t.length,i=new Array(o);i[0]=g;var s={};for(var c in n)hasOwnProperty.call(n,c)&&(s[c]=n[c]);s.originalType=e,s[u]="string"==typeof e?e:a,i[1]=s;for(var p=2;p<o;p++)i[p]=t[p];return r.createElement.apply(null,i)}return r.createElement.apply(null,t)}g.displayName="MDXCreateElement"},99837:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>i,default:()=>l,frontMatter:()=>o,metadata:()=>s,toc:()=>p});var r=t(58168),a=(t(96540),t(15680));const o={id:"consuming-kafka-topics-using-zio-streams",title:"Consuming Kafka topics using ZIO Streams"},i=void 0,s={unversionedId:"zio-kafka/consuming-kafka-topics-using-zio-streams",id:"zio-kafka/consuming-kafka-topics-using-zio-streams",title:"Consuming Kafka topics using ZIO Streams",description:"First, create a consumer using the ConsumerSettings instance:",source:"@site/docs/zio-kafka/consuming-kafka-topics-using-zio-streams.md",sourceDirName:"zio-kafka",slug:"/zio-kafka/consuming-kafka-topics-using-zio-streams",permalink:"/zio-kafka/consuming-kafka-topics-using-zio-streams",draft:!1,editUrl:"https://github.com/zio/zio/edit/series/2.x/docs/zio-kafka/consuming-kafka-topics-using-zio-streams.md",tags:[],version:"current",frontMatter:{id:"consuming-kafka-topics-using-zio-streams",title:"Consuming Kafka topics using ZIO Streams"},sidebar:"ecosystem-sidebar",previous:{title:"Getting Started",permalink:"/zio-kafka/"},next:{title:"Example of Consuming, Producing and Committing Offsets",permalink:"/zio-kafka/example-of-consuming-producing-and-committing-offsets"}},c={},p=[],m={toc:p},u="wrapper";function l(e){let{components:n,...t}=e;return(0,a.yg)(u,(0,r.A)({},m,t,{components:n,mdxType:"MDXLayout"}),(0,a.yg)("p",null,"First, create a consumer using the ConsumerSettings instance:"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-scala"},'import zio.*\nimport zio.kafka.consumer.{ Consumer, ConsumerSettings }\n\nval consumerSettings: ConsumerSettings = ConsumerSettings(List("localhost:9092")).withGroupId("group")\nval consumerScoped: ZIO[Scope, Throwable, Consumer] =\n  Consumer.make(consumerSettings)\nval consumer: ZLayer[Any, Throwable, Consumer] =\n  ZLayer.scoped(consumerScoped)\n')),(0,a.yg)("p",null,"The consumer returned from ",(0,a.yg)("inlineCode",{parentName:"p"},"Consumer.make")," is wrapped in a ",(0,a.yg)("inlineCode",{parentName:"p"},"ZLayer"),"\nto allow for easy composition with other ZIO environment components.\nYou may provide that layer to effects that require a consumer. Here's\nan example:"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-scala"},'import zio._\nimport zio.kafka.consumer._\nimport zio.kafka.serde._\n\nval data: Task[Chunk[CommittableRecord[String, String]]] =\n  Consumer.plainStream(Subscription.topics("topic"), Serde.string, Serde.string).take(50).runCollect\n    .provideSomeLayer(consumer)\n')),(0,a.yg)("p",null,"You may stream data from Kafka using the ",(0,a.yg)("inlineCode",{parentName:"p"},"plainStream")," method:"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-scala"},'import zio.Console.printLine\nimport zio.kafka.consumer._\n\nConsumer.plainStream(Subscription.topics("topic150"), Serde.string, Serde.string)\n  .tap(cr => printLine(s"key: ${cr.record.key}, value: ${cr.record.value}"))\n  .map(_.offset)\n  .aggregateAsync(Consumer.offsetBatches)\n  .mapZIO(_.commit)\n  .runDrain\n')),(0,a.yg)("p",null,"To process partitions assigned to the consumer in parallel, you may use the ",(0,a.yg)("inlineCode",{parentName:"p"},"Consumer#partitionedStream")," method, which creates a nested stream of partitions:"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-scala"},'import zio.Console.printLine\nimport zio.kafka.consumer._\n\nConsumer.partitionedStream(Subscription.topics("topic150"), Serde.string, Serde.string)\n  .flatMapPar(Int.MaxValue) { case (topicPartition, partitionStream) =>\n    ZStream.fromZIO(printLine(s"Starting stream for topic \'${topicPartition.topic}\' partition ${topicPartition.partition}")) *>\n      partitionStream\n        .tap(record => printLine(s"key: ${record.key}, value: ${record.value}")) // Replace with a custom message handling effect\n        .map(_.offset)\n  }\n  .aggregateAsync(Consumer.offsetBatches)\n  .mapZIO(_.commit)\n  .runDrain\n')),(0,a.yg)("p",null,"When using partitionedStream with ",(0,a.yg)("inlineCode",{parentName:"p"},"flatMapPar(n)"),", it is recommended to set n to ",(0,a.yg)("inlineCode",{parentName:"p"},"Int.MaxValue"),". N must be equal or greater than the number of partitions your consumer subscribes to otherwise there'll be unhandled partitions and Kafka will eventually evict your consumer."))}l.isMDXComponent=!0}}]);