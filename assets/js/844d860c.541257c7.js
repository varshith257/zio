"use strict";(self.webpackChunkzio_site=self.webpackChunkzio_site||[]).push([[83102],{15680:(e,t,n)=>{n.d(t,{xA:()=>u,yg:()=>d});var o=n(96540);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,o)}return n}function a(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,o,r=function(e,t){if(null==e)return{};var n,o,r={},i=Object.keys(e);for(o=0;o<i.length;o++)n=i[o],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(o=0;o<i.length;o++)n=i[o],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var c=o.createContext({}),m=function(e){var t=o.useContext(c),n=t;return e&&(n="function"==typeof e?e(t):a(a({},t),e)),n},u=function(e){var t=m(e.components);return o.createElement(c.Provider,{value:t},e.children)},p="mdxType",f={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},l=o.forwardRef((function(e,t){var n=e.components,r=e.mdxType,i=e.originalType,c=e.parentName,u=s(e,["components","mdxType","originalType","parentName"]),p=m(n),l=r,d=p["".concat(c,".").concat(l)]||p[l]||f[l]||i;return n?o.createElement(d,a(a({ref:t},u),{},{components:n})):o.createElement(d,a({ref:t},u))}));function d(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=n.length,a=new Array(i);a[0]=l;var s={};for(var c in t)hasOwnProperty.call(t,c)&&(s[c]=t[c]);s.originalType=e,s[p]="string"==typeof e?e:r,a[1]=s;for(var m=2;m<i;m++)a[m]=n[m];return o.createElement.apply(null,a)}return o.createElement.apply(null,n)}l.displayName="MDXCreateElement"},27281:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>a,default:()=>f,frontMatter:()=>i,metadata:()=>s,toc:()=>m});var o=n(58168),r=(n(96540),n(15680));const i={id:"example-of-consuming-producing-and-committing-offsets",title:"Example of Consuming, Producing and Committing Offsets"},a=void 0,s={unversionedId:"zio-kafka/example-of-consuming-producing-and-committing-offsets",id:"zio-kafka/example-of-consuming-producing-and-committing-offsets",title:"Example of Consuming, Producing and Committing Offsets",description:"This example shows how to consume messages from topic topica and produce transformed messages to topicb, after which consumer offsets are committed. Processing is done in chunks using ZStreamChunk for more efficiency. Please note: ZIO consumer does not support automatic offset committing. As a result, it ignores the Kafka consumer setting enable.auto.commit=true. Developers should manually commit offsets using the provided commit methods, typically after processing messages or at appropriate points in their application logic.",source:"@site/docs/zio-kafka/example-of-consuming-producing-and-committing-offsets.md",sourceDirName:"zio-kafka",slug:"/zio-kafka/example-of-consuming-producing-and-committing-offsets",permalink:"/zio-kafka/example-of-consuming-producing-and-committing-offsets",draft:!1,editUrl:"https://github.com/zio/zio/edit/series/2.x/docs/zio-kafka/example-of-consuming-producing-and-committing-offsets.md",tags:[],version:"current",frontMatter:{id:"example-of-consuming-producing-and-committing-offsets",title:"Example of Consuming, Producing and Committing Offsets"},sidebar:"ecosystem-sidebar",previous:{title:"Consuming Kafka topics using ZIO Streams",permalink:"/zio-kafka/consuming-kafka-topics-using-zio-streams"},next:{title:"Partition Assignment And Offset Retrieval",permalink:"/zio-kafka/partition-assignment-and-offset-retrieval"}},c={},m=[],u={toc:m},p="wrapper";function f(e){let{components:t,...n}=e;return(0,r.yg)(p,(0,o.A)({},u,n,{components:t,mdxType:"MDXLayout"}),(0,r.yg)("p",null,"This example shows how to consume messages from topic ",(0,r.yg)("inlineCode",{parentName:"p"},"topic_a")," and produce transformed messages to ",(0,r.yg)("inlineCode",{parentName:"p"},"topic_b"),", after which consumer offsets are committed. Processing is done in chunks using ",(0,r.yg)("inlineCode",{parentName:"p"},"ZStreamChunk")," for more efficiency. Please note: ZIO consumer does not support automatic offset committing. As a result, it ignores the Kafka consumer setting ",(0,r.yg)("inlineCode",{parentName:"p"},"enable.auto.commit=true"),". Developers should manually commit offsets using the provided commit methods, typically after processing messages or at appropriate points in their application logic."),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-scala"},'import zio.ZLayer\nimport zio.kafka.consumer._\nimport zio.kafka.producer._\nimport zio.kafka.serde._\nimport org.apache.kafka.clients.producer.ProducerRecord\n\nval consumerSettings: ConsumerSettings = ConsumerSettings(List("localhost:9092")).withGroupId("group")\nval producerSettings: ProducerSettings = ProducerSettings(List("localhost:9092"))\n\nval consumerAndProducer = \n  ZLayer.scoped(Consumer.make(consumerSettings)) ++\n    ZLayer.scoped(Producer.make(producerSettings, Serde.int, Serde.string))\n\nval consumeProduceStream = Consumer\n  .plainStream(Subscription.topics("my-input-topic"), Serde.int, Serde.long)\n  .map { record =>\n    val key: Int    = record.record.key()\n    val value: Long = record.record.value()\n    val newValue: String = value.toString\n\n    val producerRecord: ProducerRecord[Int, String] = new ProducerRecord("my-output-topic", key, newValue)\n    (producerRecord, record.offset)\n  }\n  .mapChunksZIO { chunk =>\n    val records     = chunk.map(_._1)\n    val offsetBatch = OffsetBatch(chunk.map(_._2).toSeq)\n\n    Producer.produceChunk[Any, Int, String](records) *> offsetBatch.commit.as(Chunk(()))\n  }\n  .runDrain\n  .provideSomeLayer(consumerAndProducer)\n')))}f.isMDXComponent=!0}}]);